import os
import json
import openai
from tqdm import tqdm
import numpy as np
import re
import argparse  # <--- 新增导入

# --- 配置 ---
# API凭证 (在生产环境中建议使用环境变量)
os.environ["OPENAI_API_KEY"] = "sk-DRNtKTg3hJLuU6J28jaasoxTgqKvKmqweXSViHhVAbcuEmSG"
API_BASE_URL = "https://api.chatanywhere.tech/v1"

# 评估参数
EVALUATION_MODEL = "gpt-4o-mini"

# --- GPT-4o-mini 评估模板 ---
PROMPT_TEMPLATES = {
    "step_accuracy": {
        "system": "You are a rigorous mathematical logic evaluation expert. Your task is to compare a standard problem-solving step (Gold Step) with a step generated by an AI model (Generated Step) and assess their logical consistency. Ignore minor differences in wording and focus on whether the core [PROCESS] and [CONCLUSION] are equivalent. Please return a JSON object with 'score' and 'reasoning' keys based on the following 5-point scale.\n\n5: The logic and conclusion are completely equivalent.\n4: The core logic is correct, but there are minor deviations.\n3: Contains partially correct logic, but with significant differences or errors.\n2: Superficially related, but the core logic is wrong.\n1: Completely irrelevant or contradictory.",
        "user": lambda gold, gen: {"gold_step": gold, "generated_step": gen}
    },
    "learning_adherence": {
        "system": "You are an AI learning ability analyst. Your task is to evaluate to what extent an AI model, after seeing a reference example (Retrieved Example), adopts the example's core logic or problem-solving method in its generated next step (Generated Step). Please return a JSON object with 'score' and 'reasoning' keys based on the following 5-point scale.\n\n5: Perfectly adopted the example's core method and applied it correctly.\n4: Mostly adopted the example's method, but the application was slightly inappropriate or incomplete.\n3: Adopted some elements from the example, but the core method is different.\n2: Appears to have attempted adoption, but used it completely incorrectly.\n1: Completely ignored the reference example.",
        "user": lambda retrieved, gen: {"retrieved_example": retrieved, "generated_step": gen}
    },
    "retrieval_quality": {
        "system": "You are an information retrieval quality evaluator. Your task is to judge how helpful a retrieved reference step (Retrieved Step) is for solving a target problem step (Gold Step). You need to assess their logical similarity and methodological relevance. Please return a JSON object with 'score' and 'reasoning' keys based on the following 5-point scale.\n\n5: The retrieved step provides the exact method or formula needed to solve the gold step.\n4: Provides a highly relevant method that can be used with minor adjustments.\n3: Topically relevant, offers some ideas, but is not directly applicable.\n2: Superficially related, but the core method is not applicable and could be misleading.\n1: Completely irrelevant.",
        "user": lambda retrieved, gold: {"retrieved_step": retrieved, "gold_step": gold}
    },
    "answer_accuracy": {
        "system": "You are a mathematical answer comparison expert. Your task is to determine if an AI-generated answer (Generated Answer) is mathematically equivalent to the standard answer (Gold Answer). Ignore differences in units, formatting, or trailing zeros. For example, '12' and '\\boxed{12}' are equivalent. Return a JSON object containing only one key, 'is_correct', with a value of true or false.",
        "user": lambda gold, gen: {"gold_answer": str(gold), "generated_answer": str(gen)}
    },
    "step_binary_accuracy": {
        "system": "You are a strict math logic evaluator. Compare the 'Generated Step' and 'Gold Step'. Your task is to determine if the generated step is logically and numerically equivalent to the gold step. Return a single JSON object: `{\"is_correct\": true}` or `{\"is_correct\": false}`.",
        "user": lambda gold, gen: {"gold_step": gold, "generated_step": gen}
    }
}

# --- 核心函数 ---

def get_openai_client():
    """初始化并返回OpenAI客户端。"""
    try:
        client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"), base_url=API_BASE_URL)
        return client
    except Exception as e:
        print(f"初始化OpenAI客户端时出错: {e}")
        return None

def call_evaluator(client, metric_name, **kwargs):
    """使用适当的提示调用GPT-4o-mini评估器。"""
    if not client: return None
    
    template = PROMPT_TEMPLATES[metric_name]
    user_content = template["user"](**kwargs)

    try:
        response = client.chat.completions.create(
            model=EVALUATION_MODEL,
            messages=[
                {"role": "system", "content": template["system"]},
                {"role": "user", "content": json.dumps(user_content, ensure_ascii=False)}
            ],
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        gpt_response_content = response.choices[0].message.content
        return json.loads(gpt_response_content)
    except Exception as e:
        print(f"为指标 '{metric_name}' 调用GPT时出错: {e}")
        print(f"  - 输入为: {json.dumps(user_content, ensure_ascii=False)}")
        return None

def calculate_flexible_match_counts(client, gold_steps_list, gen_steps_list):
    """
    按照用户的设计，使用灵活的N*M匹配来计算正确步骤数。
    - 遍历短列表中的每一步，在长列表中为其寻找一个（且仅一个）匹配项。
    - 返回 (二元匹配数, 评分制匹配数)
    """
    if not gen_steps_list or not gold_steps_list:
        return 0, 0

    if len(gold_steps_list) <= len(gen_steps_list):
        short_list, long_list = gold_steps_list, gen_steps_list
        short_is_gold = True
    else:
        short_list, long_list = gen_steps_list, gold_steps_list
        short_is_gold = False

    # 为两种评分方式独立计算，因为它们的匹配结果可能不同
    matched_long_indices_binary = set()
    num_binary_correct = 0
    for short_step in short_list:
        for j, long_step in enumerate(long_list):
            if j in matched_long_indices_binary:
                continue
            
            gold = short_step if short_is_gold else long_step
            gen = long_step if short_is_gold else short_step
            
            eval_res = call_evaluator(client, "step_binary_accuracy", gold=gold, gen=gen)
            if eval_res and eval_res.get("is_correct") is True:
                num_binary_correct += 1
                matched_long_indices_binary.add(j)
                break  # 贪心匹配：找到第一个就停止

    matched_long_indices_graded = set()
    num_graded_correct = 0
    for short_step in short_list:
        for j, long_step in enumerate(long_list):
            if j in matched_long_indices_graded:
                continue

            gold = short_step if short_is_gold else long_step
            gen = long_step if short_is_gold else short_step

            eval_res = call_evaluator(client, "step_accuracy", gold=gold, gen=gen)
            if eval_res and "score" in eval_res and eval_res["score"] >= 4:
                num_graded_correct += 1
                matched_long_indices_graded.add(j)
                break # 贪心匹配：找到第一个就停止

    return num_binary_correct, num_graded_correct


def main(args):
    """运行评估过程的主函数。"""
    client = get_openai_client()
    if not client:
        print("由于OpenAI客户端错误，无法开始评估。")
        return

    if not os.path.exists(args.inference_log_path):
        print(f"错误: 在 {args.inference_log_path} 找不到推理日志文件")
        return

    all_results = []
    with open(args.inference_log_path, 'r', encoding='utf-8') as f:
        for line in f:
            all_results.append(json.loads(line))

    full_evaluation_data = []
    
    for result in tqdm(all_results, desc="评估结果"):
        problem_eval = {
            "problem_id": result["problem_id"],
            "scores": {
                "learning_adherence": [],
                "final_answer_correct": None,
                "holistic_step_accuracy_binary": None,
                "holistic_step_accuracy_graded": None,
                "matched_steps_fidelity_binary": None,
                "matched_steps_fidelity_graded": None,
                "user_flex_accuracy_binary": None,
                "user_flex_accuracy_graded": None
            },
            "eval_details": []
        }

        trace = result["inference_trace"]
        gold_steps = result["gold_steps"]
        num_binary_correct_steps = 0
        num_graded_correct_steps = 0
        
        # 步骤准确率的比较，只在重叠的部分进行
        comparison_length = min(len(trace), len(gold_steps))

        for i in range(comparison_length):
            step_trace = trace[i]
            gold_step_data = gold_steps[i]

            gen_step_obj = step_trace["parsed_step_content"]
            gen_step_content = ""
            if isinstance(gen_step_obj, dict) and gen_step_obj:
                gen_step_content = list(gen_step_obj.values())[0]

            detail = {"step": step_trace["step_number"]}

            # 1a. 步骤级逻辑准确性 (1-5分制)
            eval_res_graded = call_evaluator(client, "step_accuracy", gold=gold_step_data["value"], gen=gen_step_content)
            if eval_res_graded and "score" in eval_res_graded:
                detail["step_accuracy_graded"] = eval_res_graded
                if eval_res_graded["score"] >= 4: # 大于等于4分算对
                    num_graded_correct_steps += 1

            # 1b. 步骤级二元准确率 (True/False)
            eval_res_binary = call_evaluator(client, "step_binary_accuracy", gold=gold_step_data["value"], gen=gen_step_content)
            if eval_res_binary and eval_res_binary.get("is_correct") is True:
                num_binary_correct_steps += 1
            detail["step_accuracy_binary_check"] = eval_res_binary
            
            # [修改] 只保留学习遵循度的评估
            retrieved_ctx = step_trace.get("retrieved_context_for_next_step")
            if retrieved_ctx:
                eval_res_learn = call_evaluator(client, "learning_adherence", retrieved=retrieved_ctx, gen=gen_step_content)
                if eval_res_learn and "score" in eval_res_learn:
                    problem_eval["scores"]["learning_adherence"].append(eval_res_learn["score"])
                    detail["learning_adherence"] = eval_res_learn
            
            problem_eval["eval_details"].append(detail)

        # [新逻辑] 计算两种整体步骤准确率
        denominator = max(len(trace), len(gold_steps))
        if denominator > 0:
            problem_eval["scores"]["holistic_step_accuracy_binary"] = num_binary_correct_steps / denominator
            problem_eval["scores"]["holistic_step_accuracy_graded"] = num_graded_correct_steps / denominator
        else:
            problem_eval["scores"]["holistic_step_accuracy_binary"] = 1.0
            problem_eval["scores"]["holistic_step_accuracy_graded"] = 1.0

        # [新增修复] 计算匹配步骤保真度
        # 这个指标的分母是min_denominator，也就是comparison_length
        min_denominator = comparison_length
        if min_denominator > 0:
            problem_eval["scores"]["matched_steps_fidelity_binary"] = num_binary_correct_steps / min_denominator
            problem_eval["scores"]["matched_steps_fidelity_graded"] = num_graded_correct_steps / min_denominator
        else:
            # 如果没有可比较的步骤(min=0)，保真度取决于是否有步骤存在
            # 如果两个列表都为空(max=0)，保真度可视为完美的1.0
            # 如果一个为空一个不为空，则没有匹配步骤，保真度为0.0
            if denominator == 0:
                problem_eval["scores"]["matched_steps_fidelity_binary"] = 1.0
                problem_eval["scores"]["matched_steps_fidelity_graded"] = 1.0
            else:
                problem_eval["scores"]["matched_steps_fidelity_binary"] = 0.0
                problem_eval["scores"]["matched_steps_fidelity_graded"] = 0.0


        # --- 用户设计的新指标: 柔性惩罚步骤准确率 ---
        gold_step_contents = [s["value"] for s in gold_steps]
        gen_step_contents = [list(t["parsed_step_content"].values())[0] for t in trace if isinstance(t.get("parsed_step_content"), dict) and t["parsed_step_content"]]
        
        # 1. 使用灵活匹配计算新的分子
        flex_num_binary, flex_num_graded = calculate_flexible_match_counts(client, gold_step_contents, gen_step_contents)
        
        # 2. 使用您的公式计算新的分母
        len_gold = len(gold_step_contents)
        len_gen = len(gen_step_contents)
        max_len = max(len_gold, len_gen)
        min_len = min(len_gold, len_gen)

        if len_gold > 0 and min_len > 0:
            penalty_factor = np.sqrt(max_len / min_len)
            flex_denominator = len_gold * penalty_factor
            problem_eval["scores"]["user_flex_accuracy_binary"] = flex_num_binary / flex_denominator
            problem_eval["scores"]["user_flex_accuracy_graded"] = flex_num_graded / flex_denominator
        elif len_gold == 0:
            problem_eval["scores"]["user_flex_accuracy_binary"] = 1.0 if len_gen == 0 else 0.0
            problem_eval["scores"]["user_flex_accuracy_graded"] = 1.0 if len_gen == 0 else 0.0
        else: # min_len is 0 but len_gold is not, means gen is empty
            problem_eval["scores"]["user_flex_accuracy_binary"] = 0.0
            problem_eval["scores"]["user_flex_accuracy_graded"] = 0.0


        # 3. 最终答案准确性
        if result["gold_answer"] is not None and result["final_generated_answer"] is not None:
            eval_res = call_evaluator(client, "answer_accuracy", gold=result["gold_answer"], gen=result["final_generated_answer"])
            if eval_res and isinstance(eval_res.get("is_correct"), bool):
                problem_eval["scores"]["final_answer_correct"] = eval_res["is_correct"]
        
        full_evaluation_data.append(problem_eval)

    # --- 计算并打印最终报告 ---
    total_problems = len(full_evaluation_data)
    final_answer_correct_count = sum(1 for r in full_evaluation_data if r["scores"]["final_answer_correct"] is True)
    
    # [修改] 计算两种整体步骤准确率的平均值
    avg_holistic_binary_acc = np.mean([r["scores"]["holistic_step_accuracy_binary"] for r in full_evaluation_data if r["scores"]["holistic_step_accuracy_binary"] is not None])
    avg_holistic_graded_acc = np.mean([r["scores"]["holistic_step_accuracy_graded"] for r in full_evaluation_data if r["scores"]["holistic_step_accuracy_graded"] is not None])

    # [新指标] 计算匹配步骤保真度的平均值
    avg_fidelity_binary = np.mean([r["scores"]["matched_steps_fidelity_binary"] for r in full_evaluation_data if r["scores"]["matched_steps_fidelity_binary"] is not None])
    avg_fidelity_graded = np.mean([r["scores"]["matched_steps_fidelity_graded"] for r in full_evaluation_data if r["scores"]["matched_steps_fidelity_graded"] is not None])

    # [用户设计的新指标] 计算柔性惩罚准确率的平均值
    avg_user_flex_binary = np.mean([r["scores"]["user_flex_accuracy_binary"] for r in full_evaluation_data if r["scores"]["user_flex_accuracy_binary"] is not None])
    avg_user_flex_graded = np.mean([r["scores"]["user_flex_accuracy_graded"] for r in full_evaluation_data if r["scores"]["user_flex_accuracy_graded"] is not None])


    avg_learn_adh = np.mean([s for r in full_evaluation_data for s in r["scores"]["learning_adherence"]]) if any(r["scores"]["learning_adherence"] for r in full_evaluation_data) else 0
    # [删除] 移除 avg_ret_qual 的计算，因为它不再被使用
    # avg_ret_qual = np.mean([s for r in full_evaluation_data for s in r["scores"]["retrieval_quality"]]) if any(r["scores"]["retrieval_quality"] for r in full_evaluation_data) else 0

    # [新逻辑] 计算检索影响四象限分析
    retrieval_impact = {
        "high_adherence_correct": 0, "high_adherence_incorrect": 0,
        "low_adherence_correct": 0,  "low_adherence_incorrect": 0
    }
    problems_with_retrieval = 0
    ADHERENCE_THRESHOLD = 3.5

    for r in full_evaluation_data:
        adherence_scores = r["scores"]["learning_adherence"]
        if adherence_scores:
            problems_with_retrieval += 1
            avg_adherence = np.mean(adherence_scores)
            is_correct = r["scores"]["final_answer_correct"]
            
            if avg_adherence >= ADHERENCE_THRESHOLD:
                if is_correct:
                    retrieval_impact["high_adherence_correct"] += 1
                else:
                    retrieval_impact["high_adherence_incorrect"] += 1
            else:
                if is_correct:
                    retrieval_impact["low_adherence_correct"] += 1
                else:
                    retrieval_impact["low_adherence_incorrect"] += 1

    # [新逻辑] 计算 Precision, Recall, 和 F1 Score
    tp = retrieval_impact["high_adherence_correct"]
    fp = retrieval_impact["high_adherence_incorrect"]
    fn = retrieval_impact["low_adherence_correct"]

    # 精确率 (Precision / RES)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    
    # 召回率 (Recall / RCR)
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0

    # F1 分数
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    report = {
        "total_problems_evaluated": total_problems,
        "final_answer_accuracy": f"{final_answer_correct_count / total_problems:.2%}" if total_problems > 0 else "N/A",
        "avg_holistic_step_accuracy_binary": f"{avg_holistic_binary_acc:.2%}",
        "avg_holistic_step_accuracy_graded (>=4/5)": f"{avg_holistic_graded_acc:.2%}",
        "avg_matched_steps_fidelity_binary": f"{avg_fidelity_binary:.2%}",
        "avg_matched_steps_fidelity_graded (>=4/5)": f"{avg_fidelity_graded:.2%}",
        "avg_user_designed_flex_accuracy_binary": f"{avg_user_flex_binary:.2%}",
        "avg_user_designed_flex_accuracy_graded (>=4/5)": f"{avg_user_flex_graded:.2%}",
        "avg_learning_adherence_score": f"{avg_learn_adh:.2f} / 5.0",
        "retrieval_effectiveness_score (Precision)": f"{precision:.2%}",
        "retrieval_effectiveness_score (Recall)": f"{recall:.2%}",
        "retrieval_f1_score": f"{f1_score:.2%}",
        "retrieval_impact_analysis": {
            "total_problems_with_retrieval": problems_with_retrieval,
            "analysis_threshold": f"Adherence Score >= {ADHERENCE_THRESHOLD}",
            "quadrants": {
                "high_adherence_correct (TP)": f"{tp} ({tp/problems_with_retrieval:.2%})",
                "high_adherence_incorrect (FP)": f"{fp} ({fp/problems_with_retrieval:.2%})",
                "low_adherence_correct (FN)": f"{retrieval_impact['low_adherence_correct']} ({retrieval_impact['low_adherence_correct']/problems_with_retrieval:.2%})",
                "low_adherence_incorrect (TN)": f"{retrieval_impact['low_adherence_incorrect']} ({retrieval_impact['low_adherence_incorrect']/problems_with_retrieval:.2%})"
            }
        }
    }

    print("\n--- 评估报告 ---")
    print(json.dumps(report, indent=4, ensure_ascii=False))

    # 写入日志文件
    with open(args.evaluation_report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=4, ensure_ascii=False)
    
    print(f"\n详细报告已保存至: {args.evaluation_report_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="运行Llama2 7B评估脚本")
    parser.add_argument("--inference_log_path", type=str, default="/data/yangcheng/aaai/results/inference_log_vllm_llama2.jsonl", help="推理日志文件路径")
    parser.add_argument("--evaluation_report_path", type=str, default="/data/yangcheng/aaai/results/evaluation_report_llama2.json", help="评估报告文件路径")
    args = parser.parse_args()
    main(args)